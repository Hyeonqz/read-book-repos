# 2장 카프카 설치하기
카프카 설치는 2가지 방식으로 갈린다
- 로컬에 파일 직접 설치
- docker image 를 통한 설치

필자는 docker-compose 를 통하여 구성을 하였다 <br>


## 2.3 브로커 설정하기
### 2.3.1 핵심 브로커 매개 변수
#### broker.id
broker id 는 정숫값 식별자를 가지며, 기본값은 0이지만 카프카 클러스터 안의 각 브로커 별로 전부 달라야 한다 <br>
권장사항은 호스트별로 고정된 값을 사용하는 것을 강력하게 권장한다 <br>

#### listeners
ex) PLAINTEXT://127.0.0.1/9091

#### zookeeper.connection
이것은 브로커의 메타데이터가 저장되는 주키퍼의 위치를 가리킨다 <br>

#### log.dirs
카프카는 모든 메시지를 로그 세그먼트 단위로 묶어서 log.dir 설정에 지정된 디스크 디렉토리에 저장한다 <br>

#### auto.create.topics.enable
카프카 기본 설정은 브로커가 토픽을 자동으로 생성하도록 되어있다 <br>

#### auto.leader.rebalance.enable
위 설정을 활성화 하면 리더 역할이 균등하게 분산되도록 한다. <br>
위 설정은 enable 하면 백그라운드 스레드가 시작되고 특정 브로커에 리더 역할이 할당된 파티션 비율이 <br>
leader.imbalance.per.broker.percentage 설정된 임계치를 넘어가면 리밸런싱이 발생한다 <br>

#### delete.topic.enable
위 플래그를 false 로 잡아두면 토픽 삭제 기능이 막힌다.

### 2.3.2 토픽별 기본 값
- num.partitions -> 기본값은 1
    - 토픽의 파티션 개수는 늘릴 수는 있지만 줄이기는 불가능.


#### 파티션 수는 어떻게 결정하는게 좋을끼?
이건 실무에서 정말 중요한 부분이라고 생각한다 <br>
현재 나는 브로커*3 설정을 잡아 토픽당 파티션이 9개로 구성된다 <br>

결론부터 말하면 현재 사용량이 아닌, 미래 사용량 예측값을 기준으로 계산해야 한다
- 각 브로커 배치 파티션 수 뿐만 아니라 브로커별로 사용 가능한 디스크 공간, 네트워크 대역폭 고려
- 과대 추산을 피하자
- 데이터 미러링 또한 고려
- 클라우드 서비스 사용시 IOPS(디스크 초당 입/출력) 고려

ex) 주어진 토픽에 처당 1GB 를 읽거나 쓰고자 하는데, 컨슈머 하나는 초당 50MB 만 처리가 가능하면 최소한 20개의 파티션이 필요하다 <br>

딱히 상세 정보가 없다면 파티션의 용량을 6GB로 잡는것이 좋다 <br>
작게 시작해서 필요시 확장하는 방법이 좋다 <br>

- ⭐default.replication.factor
  자동 토픽 생성 기능이 활성화 되어있으면, 이 설정은 새로 생성되는 토픽의 복제 팩터를 결정한다 <br>
  복제 팩터 값은 min.insync.replicas 설정값보다 2 큰 값으로 복제 팩터를 설정하는것이 좋다 <br>

이 값을 권장하는 이유는 레플리카 셋안에 일부러 정지시킨 레플리카와 예상치 않게 정지된 레플리카가 동시에 장애가 발생해도 안전하기 때문이다 <br>
즉 일반적인 상황이라면 최소한 3개의 레플리카를 가지는게 좋다 <br>

- ⭐log.retention.ms
  카프카가 얼마나 오랫동안 메시지를 보존해야 하는지를 지정할 때 사용된다 <br>
  기본은 1주일(168) 시간으로 설정이 되어있다 <br>
  세부 적으로 분,초 단위도 지정할 수 있다 <br>

- ⭐log.retention.bytes
  메시지 용량을 설정한다. <br>
  만약 8개의 파티션을 가진 토픽에 위 값이 1GB로 설정되어 있으면, 토픽의 최대 용량은 8GB가 된다 <br>
  모든 보존 기능은 '파티션 단위' 로 작동하지, 토픽 단위로 동작하는게 아니다 <br>
  위 값을 -1 로 설정하면 데이터가 영구적으로 보존된다 <br>

- log.segment.bytes
  앞에 설정은 로그 세그먼트에 적용되는 것이지, 메시지 각각에 적용되지 않는다 <br>
  카프카 브로커에 쓰여진 메시지는 해당 파티션의 현재 로그 세그먼트의 끝에 추가된다 <br>

기본 설정은 1GB 이며, 로그 세그먼트 가 1GB 에 다다르면, 브로커는 기존 세그먼트를 닫고 새로운 세그먼트를 생성한다 <br>
즉 작은 세그먼트 크기는 파일 I/O 작업을 더 많이 시킨다 <br>

토픽에 메시지가 뜸하게 주어지는 상황에서는 로그 세그먼트의 크기를 조절하는게 중요하다 <br>
ex) 하루에 메시지100mb, 로그 세그먼트 1gb 를 채우는데 10일이 걸린다 <br>
로그 세그먼트가 닫하기 전까지 메시지는 만료되지 않으므로 log.retention.ms 가 1주일로 잡힐 경우 닫힌 로그 세그먼트가 만료될 때 까지 <br>
실제로는 17일치 메시지가 저장될 수 있다 <br>
로그 세그먼트 닫히는 시간10일 메시지 저장시간7일 => 총 17일 <br>

- log.roll.ms
  로그 세그먼트 파일이 닫히는 시점을 제어하는 또 다른 방법

- ⭐min.insync.replicas
  최소한 2개의 레플리카를 잡아줘야 최신 상태로 프로듀서와 동기화 될 수 있다 <br>
  위 설정은 프로듀서의 ack 설정을 all 로 잡아주는것과 동일하다 <br>

위 설정 적용시 프로듀서 쓰기 작업이 성공하기 위해 최소한 2개의 레플리카를 사용한다 <br>

위 설정이 적용되어 있지 않으면 프로듀서는 쓰기 작업이 성공했다고 착각하지만, 실제로는 메시지가 유실될 수 있다 <br>
하지만 레플리카가 많을 수록 오버헤드가 많고 성능이 떨어지는 부작용이 발생할 수 있다 <br>

몇개의 메시지 유실 정도는 상관이 없다면 default 1에서 변경하지 않는 것을 권장한다 <br>

- message.max.bytes
  카프카 브로커는 쓸 수 있는 메시지 최대 크기를 제한한다. <br>
  기본값은 1mb 이다 <br>


## 2.4 하드웨어 선택하기
디스크 처리량과 용량, 메모리, 네트워크, CPU 를 감안해야 한다<br>

### 2.4.1 디스크 처리량
로그 세그먼트를 저장하는 브로커 디스크 처리량은 클라이언트의 성능에 큰 영향을 미친다 <br>
SSD 는 탐색과 접근에 들어가는 시간이 압도적으로 짧은 만큼 최고의 성능이 나온다 <br>
반면 HDD 는 더 싸고 같은 가격에 많은 용량을 제공한다 <br>

보통은 SSD 를 사용하지만, 자주 사용할 일이 없는 데이터를 처리 할때는 HDD 를 사용하기도 한다 <br>

### 2.4.2 디스크 용량
디스크 용량은 특정한 시점에 얼마나 많은 메시지들이 보존되어야 하는지에 따라 결정된다 <br>

### 2.4.3 메모리
카프카 컨슈머는 프로듀서가 추가한 메시지를 바로 뒤에서 쫓아가는 식으로 파티션의 맨 끝에서 메시지를 읽어오는 것이 보통이다 <br>
위 상황에서 최적의 작동은 시스템의 페이지 캐시에 저장되어 있는 메시지들을 컨슈머가 읽어 오는 것이 된다 <br>
시스템에 페이지 캐시로 사용할 수 있는 메모리를 더 할당해 줌으로써 컨슈머 클라이언트 성능을 향상시킬 수 있다 <br>

카프카 그 자체는 JVM 에 많은 힙 메모리를 필요로 하지 않는다 <br>
초당 150,000개의 메시지에 200MB의 데이터 속도를 처리하는 브로커일지라도 5GB 힙과 함께 돌아간다 <br>

시스템 메모리의 나머지 영역은 페이지 캐시로 사용되어 시스템이 사용중인 로그 세그먼트를 캐시하도록 함으로써 카프카의 성능을 향상시킬 수 있는 것이다 <br>
이것은 카프카를 하나의 시스템에서 다른 애플리케이션과 함께 운영하는 것을 권장하는 않는 이유다 -> 독립 서버에서 운영을 하자..! <br>
페이지 캐시를 나눠서 쓰게 되기 때문? -> 이게 무슨 말인지 잘 모르겠음 <br>


### 2.4.4 네트워크
사용 가능한 네트워크 대역폭은 카프카가 처리할 수 있는 트래픽의 최대량을 결정한다 <br>
이것은 디스크 용량과 함께 클러스터 크기를 결정하는 가장 결정적인 요인이라고 한다 <br>

카프카는 다수의 컨슈머를 동시에 지원하기 때문에 인입되는 네트워크 사용량과 유출되는 네트워크 사용량 사이에 불균형이 생길 수 밖에 없다 <br>
ex) 프로듀서는 주어진 토픽에 초당 1MB 를 쓰지만, 해당 토픽에 여러 컨슈머가 다수 붙음으로써 유출되는 네트워크 사용량이 훨씬 많아질 수 있다 <br>

네트워크 인터페이스가 포화상태에 빠질 경우, 클러스터 내부의 복제 작업이 밀려서 클러스터가 취약한 상태로 빠지는 사태가 나올 수 있다 <br>
네트워크 문제가 불거져 나오는 것을 방지하기 위해서는 최소한 10GB 이상을 처리할 수 있는 네트워크 인터페이스 카드를 사용할 것을 권장한다 <br>

### 2.4.5 CPU
카프카 클러스터가 크지 않은 이상, cpu 는 디스크,메모리 만큼 중요하지는 않다 <br>

### 정리
- 디스크
- 메모리
- 네트워크
- cpu

위 순으로 하드웨어를 고려하면 좋을 듯 합니다.

## 2.5 클라우드에서 카프카 사용하기
- aws
  - aws ebs
- azure
- gcp
- ncp

## 2.6 카프카 클러스터 설정하기
카프카 클러스터의 장점은 부하를 다수의 서버로 확장할 수 있다 <br>
그 다음 이점은 복제를 사용함으로써 단일 시스템 장애에서 발생할 수 있는 데이터 유실을 방지할 수 있다 <br>

### 2.6.1 ~ 2.6.2 브로커 설정
현재는 파티션 레플리카 개수를 브로커당 14000개, 클러스터 당 100만개 이하로 유지할 것을 권장한다 <br>

다수의 브로커가 하나의 클러스터를 이루기 위해 설정해야 하는 것은 2가지 이다.
- 모든 브로커들이 동일한 zookeeper.connect 설정 값
  - 이것은 클러스터가 메타데이터를 저장하는 주키퍼 앙상블과 경로를 지정한다.
- 클러스터 안의 모든 브로커가 유일한 broker.id 설정 값
  - 만약 동일한 broker.id 를 가지면 늦게 들어오는 쪽에 에러가 났다는 로그가 찍힌다.

### 2.6.3 운영체제 튜닝
#### 가상 메모리
보통 처리량이 중요한 어플리케이션은 메모리 스와핑을 막는 것이 좋다 <br>
메모리의 페이지가 디스크로 스와핑되는 과정에서 발생하는 리소스는 카프카 성능에서도 많은 영향을 준다 <br>

카프카는 프로듀서의 요청에 빠르게 응답하기 위해 디스크 입출력 성능에 의존한다 <br>

#### 디스크
저장 장치 하드웨어를 선택하는 것과 RAID 설정 다음으로 성능에 큰 영향을 미칠 수 있는 것이 디스크의 파일 시스템이다 <br>

## 2.7 프로덕션 환경에서의 고려 사항
### 2.7.1 GC 옵션
애플리케이션이 메모리를 어떻게 사용하는지에 대해 상세히 알아야 할 뿐만 아니라 상당한 시행착오 역시 수반하는 일이기 때문이다 <br>
현재 카프카에는 GIGC 를 기본 가비시 수집기로 사용할 것을 권장한다 <br>

카프카 브로커는 힙 메모리를 상당히 효율적으로 사용할 뿐만 아니라 가비지 수집의 대상이 되는 객체 역시 가능한 적게 생성하기 때문에 설정들을 낮게 잡아도 좋다 <br>

### 2.7.3 주키퍼 공유하기
카프카는 브로커,토픽,파티션에 대한 메타데이터 정보를 저장하기 위해 주키퍼를 사용한다 <br>
컨슈머 그룹 멤버나 카프카 클러스터 자체에 변동이 있을 때만 주키퍼 쓰기 작업이 이루어 진다 <br>

#### 카프카 컨슈머, 툴, 주키퍼
시간이 흐르면서 주키퍼에 대한 의존도는 줄어들고 있다 <br>
버전 2.8.0 부터는 주키퍼 없이 카프카가 동작할 수 있으며, 3.3.0 부터는 정식 버전이 출시되었다 <br>

이에 따라 --zookeeper 옵션들이 --bootstrap-server 옵션으로 변경이 되었다 <br>


